{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJchgxYHViTNCBM6nJ+2eQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garry-p12/PhrasalSegmentation/blob/main/DSC253_HW2_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdTs2l7wB07K",
        "outputId": "bb366187-748b-4eee-f6da-789f12eb98bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AutoPhrase'...\n",
            "remote: Enumerating objects: 967, done.\u001b[K\n",
            "remote: Counting objects: 100% (137/137), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 967 (delta 66), reused 122 (delta 60), pack-reused 830 (from 1)\u001b[K\n",
            "Receiving objects: 100% (967/967), 199.80 MiB | 17.71 MiB/s, done.\n",
            "Resolving deltas: 100% (438/438), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/shangjingbo1226/AutoPhrase.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd AutoPhrase/ && ./auto_phrase.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJM3rs31B6Z0",
        "outputId": "0d04eedc-5a68-4568-e23b-210284670582"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m===Compilation===\u001b[m\n",
            "mkdir -p bin\n",
            "g++ -std=c++11 -Wall -O3 -msse2  -fopenmp  -I.. -pthread -lm -Wno-unused-result -Wno-sign-compare -Wno-unused-variable -Wno-parentheses -Wno-format -o bin/segphrase_train src/main.cpp\n",
            "g++ -std=c++11 -Wall -O3 -msse2  -fopenmp  -I.. -pthread -lm -Wno-unused-result -Wno-sign-compare -Wno-unused-variable -Wno-parentheses -Wno-format -o bin/segphrase_segment src/segment.cpp\n",
            "\u001b[32m===Downloading Toy Dataset===\u001b[m\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  198M  100  198M    0     0  3819k      0  0:00:53  0:00:53 --:--:-- 4070k\n",
            "\u001b[32m===Tokenization===\u001b[m\n",
            "\n",
            "real\t4m49.176s\n",
            "user\t7m48.369s\n",
            "sys\t0m17.651s\n",
            "Detected Language: EN\u001b[0K\n",
            "Current step: Tokenizing wikipedia phrases...\u001b[0K\n",
            "No provided expert labels.\u001b[0K\n",
            "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 3253k  100 3253k    0     0  1845k      0  0:00:01  0:00:01 --:--:-- 1845k\n",
            "english parameter file (Linux, UTF8) installed.\n",
            "\n",
            "\u001b[32m===AutoPhrasing===\u001b[m\n",
            "=== Current Settings ===\n",
            "Iterations = 2\n",
            "Minimum Support Threshold = 10\n",
            "Maximum Length Threshold = 6\n",
            "POS-Tagging Mode Enabled\n",
            "Number of threads = 10\n",
            "Labeling Method = DPDN\n",
            "\tAuto labels from knowledge bases\n",
            "\tMax Positive Samples = -1\n",
            "=======\n",
            "Loading data...\n",
            "# of total tokens = 111149629\n",
            "max word token id = 553924\n",
            "# of documents = 5547032\n",
            "# of distinct POS tags = 57\n",
            "Mining frequent phrases...\n",
            "selected MAGIC = 553933\n",
            "# of frequent phrases = 1801956\n",
            "Extracting features...\n",
            "Constructing label pools...\n",
            "\tThe size of the positive pool = 32635\n",
            "\tThe size of the negative pool = 1762859\n",
            "# truth patterns = 201855\n",
            "Estimating Phrase Quality...\n",
            "Segmenting...\n",
            "Rectifying features...\n",
            "Estimating Phrase Quality...\n",
            "Segmenting...\n",
            "Dumping results...\n",
            "Done.\n",
            "\n",
            "real\t37m55.999s\n",
            "user\t58m56.505s\n",
            "sys\t3m1.182s\n",
            "\u001b[32m===Saving Model and Results===\u001b[m\n",
            "\u001b[32m===Generating Output===\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd AutoPhrase/ && ./phrasal_segmentation.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecZnWBxHCdOz",
        "outputId": "f336e91d-96d2-422c-ef5f-0ae00d59e316"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m===Compilation===\u001b[m\n",
            "\u001b[32m===Tokenization===\u001b[m\n",
            "\n",
            "real\t1m46.946s\n",
            "user\t2m23.844s\n",
            "sys\t0m15.585s\n",
            "Detected Language: EN\u001b[0K\n",
            "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
            "\n",
            "\u001b[32m===Phrasal Segmentation===\u001b[m\n",
            "=== Current Settings ===\n",
            "Segmentation Model Path = models/DBLP/segmentation.model\n",
            "After the phrasal segmentation, only following phrases will be highlighted with <phrase> and </phrase>\n",
            "\tQ(multi-word phrases) >= 0.500000\n",
            "\tQ(single-word phrases) >= 0.800000\n",
            "=======\n",
            "POS guided model loaded.\n",
            "# of loaded patterns = 721623\n",
            "# of loaded truth patterns = 234490\n",
            "POS transition matrix loaded\n",
            "Phrasal segmentation finished.\n",
            "   # of total highlighted quality phrases = 24355353\n",
            "   # of total processed sentences = 13186794\n",
            "   avg highlights per sentence = 1.84695\n",
            "\n",
            "real\t4m29.229s\n",
            "user\t4m19.691s\n",
            "sys\t0m6.549s\n",
            "\u001b[32m===Generating Output===\u001b[m\n",
            "\n",
            "real\t1m55.947s\n",
            "user\t1m49.747s\n",
            "sys\t0m6.818s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def parse_segmented_corpus(input_file, output_file):\n",
        "    sentences = []\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            # Replace phrases with a single token (remove <phrase> tags and replace spaces with underscores)\n",
        "            cleaned_line = re.sub(r'<phrase_Q=[^>]+>([^<]+)</phrase>', lambda m: m.group(1).replace(' ', '_'), line)\n",
        "            sentences.append(cleaned_line.split())\n",
        "\n",
        "    # Write the parsed corpus into a new file\n",
        "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
        "        for sentence in sentences:\n",
        "            out_f.write(\" \".join(sentence) + \"\\n\")\n",
        "\n",
        "    return sentences\n",
        "\n",
        "parsed_sentences = parse_segmented_corpus('/content/segmentation.txt', 'parsed_corpus.txt')\n",
        "\n",
        "print(parsed_sentences[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC8e3jkRDPIW",
        "outputId": "cac862a4-6b01-4bf3-85c0-d4310c1e43b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['OQL[C++]:', 'Extending', 'C++', 'with', 'an', 'Object', 'Query', 'Capability.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(parsed_sentences[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmUZZuGxRvdG",
        "outputId": "3210b14a-fb1d-4258-80c0-a7d56d69ec75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Where', 'Object-Oriented', 'DBMSs', 'Should', 'Do', 'Better:', 'A', 'Critique', 'Based', 'on', 'Early_Experiences.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(parsed_sentences, vector_size=100, window=5, min_count=5, workers=4)"
      ],
      "metadata": {
        "id": "M0XyedQNYl5X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"word2vec_phrases_parsed.model\")"
      ],
      "metadata": {
        "id": "WMry8DHiY30d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "model = Word2Vec.load(\"/content/word2vec_phrases_parsed.model\")\n",
        "\n",
        "phrases = list(model.wv.index_to_key)\n",
        "embeddings = np.array([model.wv[phrase] for phrase in phrases])"
      ],
      "metadata": {
        "id": "_ZiFKgAUZkr9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrases[:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i50dxJbsmrFQ",
        "outputId": "3ace85fd-75d8-4f9c-c616-fdf4a082c9e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'of',\n",
              " 'and',\n",
              " 'a',\n",
              " 'to',\n",
              " '.',\n",
              " 'in',\n",
              " 'for',\n",
              " 'is',\n",
              " 'that',\n",
              " 'on',\n",
              " 'The',\n",
              " 'with',\n",
              " 'are',\n",
              " 'by',\n",
              " 'this',\n",
              " 'as',\n",
              " 'an',\n",
              " 'we',\n",
              " 'be',\n",
              " 'We',\n",
              " 'A',\n",
              " 'can',\n",
              " 'In',\n",
              " 'which',\n",
              " 'from',\n",
              " 'This',\n",
              " 'paper',\n",
              " 'using',\n",
              " 'system']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Set number of clusters to 6\n",
        "kmeans = KMeans(n_clusters=6, random_state=42)\n",
        "kmeans.fit(embeddings)\n",
        "\n",
        "# Get cluster labels\n",
        "kmeans_labels = kmeans.labels_"
      ],
      "metadata": {
        "id": "3mwkxLJtbbTz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Fit a Gaussian Mixture Model with 6 components (clusters)\n",
        "gmm = GaussianMixture(n_components=6, random_state=42)\n",
        "gmm.fit(embeddings)\n",
        "\n",
        "# Get cluster labels\n",
        "gmm_labels = gmm.predict(embeddings)\n"
      ],
      "metadata": {
        "id": "oBcVdafsb9Z_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_cluster_phrases(phrases, labels, method_name, cluster_num=6, num_phrases=20):\n",
        "    for i in range(cluster_num):\n",
        "        print(f\"\\nCluster {i+1} - {method_name}:\")\n",
        "        cluster_phrases = [phrases[j] for j in range(len(phrases)) if labels[j] == i]\n",
        "        print(cluster_phrases[:num_phrases])  # Print top 20 phrases from each cluster\n",
        "\n",
        "# Print results for K-Means\n",
        "print_cluster_phrases(phrases, kmeans_labels, 'K-Means')\n",
        "\n",
        "print_cluster_phrases(phrases, gmm_labels, 'GMM')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R6XK3tXbfnF",
        "outputId": "b03b1402-0495-4128-cc1d-7c3e34e37d2a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cluster 1 - K-Means:\n",
            "['System', 'Design', 'Analysis', 'Approach', 'New', 'Model', 'Systems.', 'Algorithm', 'Distributed', 'Data', 'Information', 'System.', 'Dynamic', 'Learning', 'Efficient', 'Parallel', 'Application', 'Method', 'Modeling', 'Mobile']\n",
            "\n",
            "Cluster 2 - K-Means:\n",
            "['-', 'Simulation', 'recently', 'Inc.', 'VLSI', 'FPGA', 'IEEE', '&', 'et_al.', 'named', 'multiprocessor', 'First', ':', 'Association_for_Computing_Machinery.', 'Over', 'IBM', 'protocol,', 'inspired', 'conjunction', 'fault-tolerant']\n",
            "\n",
            "Cluster 3 - K-Means:\n",
            "['the', 'of', 'a', 'to', 'for', 'is', 'that', 'on', 'The', 'with', 'are', 'by', 'as', 'an', 'we', 'be', 'We', 'A', 'can', 'which']\n",
            "\n",
            "Cluster 4 - K-Means:\n",
            "['.', 'through', 'called', 'system.', 'systems.', 'applications.', 'via', 'problem.', 'presented.', 'data.', 'networks.', 'model.', 'time.', 'approach.', 'environment.', 'process.', 'problems.', 'performance.', 'algorithm.', 'network.']\n",
            "\n",
            "Cluster 5 - K-Means:\n",
            "['Using', 'An', 'On', 'Based', 'presents', 'With', 'proposes', 'Experimental', 'Some', 'Two', 'From', 'Results', 'introduces', 'Its', 'combines', 'Experiments', 'Through', 'Several', 'Both', 'All']\n",
            "\n",
            "Cluster 6 - K-Means:\n",
            "['and', 'in', 'this', 'In', 'This', 'paper', 'system', 'have', 'has', 'it', 'such', 'new', 'our', 'approach', 'used', 'data', 'these', 'been', 'their', 'use']\n",
            "\n",
            "Cluster 1 - GMM:\n",
            "['Forum.', 'Reviews.', 'Program_Committee.', 'Fault_Diagnosis.', 'Belief_Revision.', 'Announcements.', 'Global_Optimization.', 'Corrigendum:', 'Simplification.', 'Committees.', 'Optimisation.', 'Bi-directional', 'Normalization.', 'Predictions.', 'Web_Service_Composition.', 'Schedules.', 'Abstracts.', 'Fuzzy_Clustering.', 'Document_Retrieval.', 'Combinatorial_Optimization.']\n",
            "\n",
            "Cluster 2 - GMM:\n",
            "['Guest_Editorial.', 'Organizing_Committee.', 'Erratum.', 'Multiple_Sequence_Alignment.', 'Online_Learning.', 'Approximate_Reasoning.', 'Whither', 'Digital_Watermarking.', 'Program_Slicing.', 'Fault-Tolerance.', 'Program_Synthesis.', 'Timing.', 'Computer_Security.', 'Plan_Recognition.', 'Texture_Classification.', 'Soft_Computing.', 'Morphing.', 'Oblivious_Transfer.', 'Fuzzy_Control.', 'Cloud_Computing.']\n",
            "\n",
            "Cluster 3 - GMM:\n",
            "['Complex', 'Generic', 'Recursive', 'Modular', 'Collaborative', 'Weighted', 'Constrained', '(Extended_Abstract).', 'On-Line', 'Neural', 'Large-Scale', 'Independent', 'Inference', 'Concept', 'Deterministic', 'Sparse', 'Content', 'Bounded', 'Representations', 'Interval']\n",
            "\n",
            "Cluster 4 - GMM:\n",
            "['Editorial.', 'Preface.', 'Foreword.', 'Higher_Order', 'Data-Driven', 'Multi-dimensional', 'Non-Linear', 'Top-Down', 'Slicing', 'Data_Structures.', 'Synchronized', 'Relaxation', 'Shadow', 'Indirect', 'Operation.', 'Second-Order', 'Procedures.', 'Multi-scale', 'Simplifying', 'Repair']\n",
            "\n",
            "Cluster 5 - GMM:\n",
            "['Title.', 'Book_Reviews.', 'Über', 'Anwendungen.', 'Einsatz', 'New_Products.', 'Konzepte', 'Bookshelf.', 'Über_die', 'Ergebnisse', 'Neue', 'Sicherheit', 'Modellierung_von', 'Grundlagen', 'Visualisierung', 'Article_Summaries.', 'Wissensbasierte', 'Elektronische', 'Probleme', 'Guest_editorial.']\n",
            "\n",
            "Cluster 6 - GMM:\n",
            "['the', 'of', 'and', 'a', 'to', '.', 'in', 'for', 'is', 'that', 'on', 'The', 'with', 'are', 'by', 'this', 'as', 'an', 'we', 'be']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zvcXaUsxb2cB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}